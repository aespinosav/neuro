{
 "metadata": {
  "name": "",
  "signature": "sha256:cab246c548624b6ea3d0df6b2fd42eee0dfdefbd1e9bfeebeff688c5069933df"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "A Kernel-Based Calculation of Information on a Metric Space"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook we will be following Tobin & Houghton's approach to measure the mutual information between stimuli and spiking responses of sensory neurons.\n",
      "\n",
      "The type of problem that motivates this endeavour is understanding the way neurons represent the stimuli they are presented with. We want to understand the neural code. \n",
      "\n",
      "Let us consider the way some experiments on sensory pathway electrophysiology are carried out. Neurons are presented with stimuli that are drawn from a discrete space $\\mathcal{S}$ and the response by the neuron is reccorded. The structure of this response can be broken down to a series of times at which there is a spike in the action potential of the neuron. It is clear, then, that the space of mathematical objects taht could be used to represent all possible responses is not discrete. A spike train can have different numbers of actual spikes, and the time at which they occur takes values in the Real Numbers. We will call the space of responses $\\mathcal{R}$.\n",
      "\n",
      "The problem is that because of the structure of spike trains, the space in which they \"live\" in is not a manifold. Even if the individual spikes could be thought of as coordinates, this idea breaks down when we consider that responses to the same stimulus can contain a different number of spikes. However, all is not lost, since the spike train space *does* have a metric.\n",
      "\n",
      "To calculate the information between a stimuli and the responses we need to treat them as random variables. The stimulus $S$ takes values in $\\mathcal{S}$ and the response $R$ takes values in the spkie train space $\\mathcal{R}$. Since one of the spaces is discrete and the other continuous there are two possible paths to take: one is to discretise $\\mathcal{R}$ (which is the prevalent method), the other is to use differential mutual information. The mutual information between our continuous and discrete random variables is,\n",
      "\\begin{equation}\n",
      "I(R, S) = \\sum_{s\\in\\mathcal{S}} \\int_{\\mathcal{R}} p(r, s) \\log_2 \\dfrac{p(r, s)}{p(r)p(s)} dr.\n",
      "\\end{equation}\n",
      "\n",
      "It is here that we run into another problem, the differential $dr$ is the measure on $\\mathcal{R}$, but since $\\mathcal{R}$ does not have a coordinate system it is not clear what this measure is. The solution is that since $R$ is a random variable, the probability distribution of $R$ on $\\mathcal{R}$ induces a statistical measure on the space. The volume of a region $\\mathcal{D} \\subset \\mathcal{R}$ can be identified with $P(x \\in \\mathcal{D})$. So in order to carry out the integration, this is the measure that will be used.\n",
      "\n",
      "Using Bayes' theorem, we can convert the previous expresion for the mutual information into,\n",
      "\n",
      "\\begin{equation}\n",
      "I(R, S) = \\sum_{s\\in\\mathcal{S}} \\int_{\\mathcal{R}} p(r, s) \\log_2 \\dfrac{p(r|s)}{p(r)} dr.\n",
      "\\end{equation}\n",
      "\n",
      "What we have to do now is use the fact tha the measure is given by the probability distribution $p(r)$. Therefore we have for a region $\\mathcal{D} \\subset \\mathcal{R}$,\n",
      "\\begin{equation}\n",
      "\\text{vol}(\\mathcal{D}) = \\int_{\\mathcal{D}}p(r)dr = \\int_{\\mathcal{D}} d\\beta.\n",
      "\\end{equation} This results in,\n",
      "\\begin{equation}\n",
      "d\\beta = p(r)dr.\n",
      "\\end{equation}\n",
      "\n",
      "The probability density, under the new measure is given by,\n",
      "\\begin{equation}\n",
      "p_{\\beta}(r) = \\dfrac{p(r)}{J(r)},\n",
      "\\end{equation}\n",
      "where $J(r)$ is the jacobian of the transformation that changes the measure evaluated at $r$. In our case $J(r) = d\\beta / dr$. Therefore,\n",
      "\\begin{equation}\n",
      "p_{\\beta}(r) = \\dfrac{p(r)}{d\\beta / dr}\n",
      "\\end{equation}\n",
      "\n",
      "By introducing this new measure baesed on the probability density, we have forced (in the previous integral) the probability distribution relative to it, to be,\n",
      "\\begin{equation}\n",
      "p_{\\beta}(r) = 1.\n",
      "\\end{equation}\n",
      "It might seem strange that the probability density is now uniform on the space of responses, however, this is taken care of by the scaling factor $d\\beta / dr$.\n",
      "\n",
      "We can now simplify the expression for the mutual information even further,\n",
      "\\begin{equation}\n",
      "I(R, S) = \\sum_{s\\in\\mathcal{S}} \\int_{\\mathcal{R}} p_{\\beta}(r, s) \\log_2 p_\\beta(r|s) d\\beta.\n",
      "\\end{equation}\n",
      "\n",
      "We can also express the mutual information as the expectation of $p_\\beta(r|s)$, whose Monte-Carlo estimate is given by,\n",
      "\\begin{equation}\n",
      "I(R;S)=<\\log_2p_\\beta(r|s)> \\approx \\dfrac{1}{n_r}\\sum_i \\log_2 p_\\beta(r_i|s_i),\n",
      "\\end{equation} where $n_r$ is the number of responses and $s_i$ is the stimulus corresponding to the response $r_i$. What we will actaully be estimating using KDE is $p_{\\beta}(r_i|s_i)$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Estimating the conditional distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The fact that the measure is a statistical measure, means that the width of the kernel is determined by the ammount of data points. This might seem strange, since when one uses KDE in a space where the measure is derived from the coordinates the size of the kernel is fixed by the bandwidth $h$, in our case, the size of the kernel varies with the density of the data points across $\\mathcal{R}$ for a fixed bandwidth. To be able to make this estimate, we will define the kernel in terms of the values it has at points $r^{\\prime}$ which will be determined to be in the support of $k_h$ according to our metric.\n",
      "\n",
      "What we do want to remain constant is the value of the integral,\n",
      "\\begin{equation}\n",
      "\\int_{\\mathcal{S}(r;h)}k_h(r^{\\prime},r)d\\beta,\n",
      "\\end{equation} and since the kernel can be thought of as a probability density on its own, we will require that it integrates to unity,\n",
      "\\begin{equation}\n",
      "\\int_{\\mathcal{S}(r;h)}k_h(r^{\\prime},r)d\\beta = 1.\n",
      "\\end{equation}\n",
      "\n",
      "Basically, when we specify the bandwidth of the kenrel we are defining over how many of our data points the kernel has a non-zero value (since we will be using the square kernel), more specifically we are specifying which proportion of the data points will be used in our Monte-Carlo estimate. We are also defining the value it has at each of these points, as we have added the restriction that the kernel must integrate to 1.\n",
      "\n",
      "For a bandwidth $k$, which must be in the interval $[0,1]$ since we are dealing with a probabilistic measure, the volume of the region on which the kernel does not vanish is estimated by the fraction of data points in it given by $h$. However, since this number must be an integer we will take the *floor* function of $hn_r$ as the number of points in the support of the kernel, we will call this number $n_h$.\n",
      "\n",
      "The uniform kernel around a point $r_i$ is defined as,\n",
      "\\begin{equation}\n",
      "k_h(r, r_i) = \\left\\{\n",
      "\\begin{matrix}\n",
      "\\frac{1}{n_h}, & r \\,\\text{is one of the $n_h$ closest points to $r_i$}\\\\\n",
      "0 & \\text{otherwise}.\n",
      "\\end{matrix}\n",
      "\\right.\n",
      "\\end{equation}\n",
      "\n",
      "Whether $r_i$ is one of the $n_h$ closest points to $r$ is determined by the metric on $\\mathcal{R}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Algorithm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}