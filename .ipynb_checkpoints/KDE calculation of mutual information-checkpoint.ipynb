{
 "metadata": {
  "name": "",
  "signature": "sha256:c8e0deb6ee9926a3df12070a9a55eb9e95acfa4fdc7a7c3e54af4f18ced603e6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "A Kernel-Based Calculation of Information on a Metric Space"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook we will be following Tobin & Houghton's approach to measure the mutual information between stimuli and spiking responses of sensory neurons.\n",
      "\n",
      "The type of problem that motivates this endeavour is understanding the way neurons represent the stimuli they are presented with. We want to understand the neural code. \n",
      "\n",
      "Let us consider the way some experiments on sensory pathway electrophysiology are carried out. Neurons are presented with stimuli that are drawn from a discrete space $\\mathcal{S}$ and the response by the neuron is reccorded. The structure of this response can be broken down to a series of times at which there is a spike in the action potential of the neuron. It is clear, then, that the space of mathematical objects taht could be used to represent all possible responses is not discrete. A spike train can have different numbers of actual spikes, and the time at which they occur takes values in the Real Numbers. We will call the space of responses $\\mathcal{R}$.\n",
      "\n",
      "The problem is that because of the structure of spike trains, the space in which they \"live\" in is not a manifold. Even if the individual spikes could be thought of as coordinates, this idea breaks down when we consider that responses to the same stimulus can contain a different number of spikes. However, all is not lost, since the spike train space *does* have a metric.\n",
      "\n",
      "To calculate the information between a stimuli and the responses we need to treat them as random variables. The stimulus $S$ takes values in $\\mathcal{S}$ and the response $R$ takes values in the spkie train space $\\mathcal{R}$. Since one of the spaces is discrete and the other continuous there are two possible paths to take: one is to discretise $\\mathcal{R}$ (which is the prevalent method), the other is to use differential mutual information. The mutual information between our continuous and discrete random variables is,\n",
      "\\begin{equation}\n",
      "I(R, S) = \\sum_{s\\in\\mathcal{S}} \\int_{\\mathcal{R}} p(r, s) \\log_2 \\dfrac{p(r, s)}{p(r)p(s)} dr.\n",
      "\\end{equation}\n",
      "\n",
      "It is here that we run into another problem, the differential $dr$ is the measure on $\\mathcal{R}$, but since $\\mathcal{R}$ does not have a coordinate system it is not clear what this measure is. The solution is that since $R$ is a random variable, the probability distribution of $R$ on $\\mathcal{R}$ induces a statistical measure on the space. The volume of a region $\\mathcal{D} \\subset \\mathcal{R}$ can be identified with $P(x \\in \\mathcal{D})$. So in order to carry out the integration, this is the measure that will be used.\n",
      "\n",
      "Using Bayes' theorem, we can convert the previous expresion for the mutual information into,\n",
      "\n",
      "\\begin{equation}\n",
      "I(R, S) = \\sum_{s\\in\\mathcal{S}} \\int_{\\mathcal{R}} p(r, s) \\log_2 \\dfrac{p(r|s)}{p(r)} dr.\n",
      "\\end{equation}\n",
      "\n",
      "What we have to do now is use the fact tha the measure is given by the probability distribution $p(r)$. Therefore we have for a region $\\mathcal{D} \\subset \\mathcal{R}$,\n",
      "\\begin{equation}\n",
      "\\text{vol}(\\mathcal{D}) = \\int_{\\mathcal{D}}p(r)dr = \\int_{\\mathcal{D}} d\\beta.\n",
      "\\end{equation} This results in,\n",
      "\\begin{equation}\n",
      "d\\beta = p(r)dr.\n",
      "\\end{equation}\n",
      "\n",
      "The probability density, under the new measure is given by,\n",
      "\\begin{equation}\n",
      "p_{\\beta}(r) = \\dfrac{p(r)}{J(r)},\n",
      "\\end{equation}\n",
      "where $J(r)$ is the jacobian of the transformation that changes the measure evaluated at $r$. In our case $J(r) = d\\beta / dr$. Therefore,\n",
      "\\begin{equation}\n",
      "p_{\\beta}(r) = \\dfrac{p(r)}{d\\beta / dr}\n",
      "\\end{equation}\n",
      "\n",
      "By introducing this new measure baesed on the probability density, we have forced (in the previous integral) the probability distribution relative to it, to be,\n",
      "\\begin{equation}\n",
      "p_{\\beta}(r) = 1.\n",
      "\\end{equation}\n",
      "It might seem strange that the probability density is now uniform on the space of responses, however, this is taken care of by the scaling factor $d\\beta / dr$.\n",
      "\n",
      "We can now simplify the expression for the mutual information even further,\n",
      "\\begin{equation}\n",
      "I(R, S) = \\sum_{s\\in\\mathcal{S}} \\int_{\\mathcal{R}} p_{\\beta}(r, s) \\log_2 p_\\beta(r|s) d\\beta.\n",
      "\\end{equation}\n",
      "\n",
      "We can also express the mutual information as the expectation of $p_\\beta(r|s)$, whose Monte-Carlo estimate is given by,\n",
      "\\begin{equation}\n",
      "I(R;S)=<\\log_2p_\\beta(r|s)> \\approx \\dfrac{1}{n_r}\\sum_i \\log_2 p_\\beta(r_i|s_i),\n",
      "\\end{equation} where $n_r$ is the number of responses and $s_i$ is the stimulus corresponding to the response $r_i$. What we will actaully be estimating using KDE is $p_{\\beta}(r_i|s_i)$.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Estimating the conditional distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The fact that the measure is a statistical measure, means that the width of the kernel is determined by the ammount of data points. This might seem strange, since when one uses KDE in a space where the measure is derived from the coordinates the size of the kernel is fixed by the bandwidth $h$, in our case, the size of the kernel varies with the density of the data points across $\\mathcal{R}$ for a fixed bandwidth. To be able to make this estimate, we will define the kernel in terms of the values it has at points $r^{\\prime}$ which will be determined to be in the support of $k_h$ according to our metric.\n",
      "\n",
      "What we do want to remain constant is the value of the integral,\n",
      "\\begin{equation}\n",
      "\\int_{\\mathcal{S}(r;h)}k_h(r^{\\prime},r)d\\beta,\n",
      "\\end{equation} and since the kernel can be thought of as a probability density on its own, we will require that it integrates to unity,\n",
      "\\begin{equation}\n",
      "\\int_{\\mathcal{S}(r;h)}k_h(r^{\\prime},r)d\\beta = 1.\n",
      "\\end{equation}\n",
      "\n",
      "Basically, when we specify the bandwidth of the kenrel we are defining over how many of our data points the kernel has a non-zero value (since we will be using the square kernel), more specifically we are specifying which proportion of the data points will be used in our Monte-Carlo estimate. We are also defining the value it has at each of these points, as we have added the restriction that the kernel must integrate to 1.\n",
      "\n",
      "For a bandwidth $k$, which must be in the interval $[0,1]$ since we are dealing with a probabilistic measure, the volume of the region on which the kernel does not vanish is estimated by the fraction of data points in it given by $h$. However, since this number must be an integer we will take the *floor* function of $hn_r$ as the number of points in the support of the kernel, we will call this number $n_h$.\n",
      "\n",
      "The value the uniform kernel around a point $r_i$ takes at point $r$, is defined as,\n",
      "\\begin{equation}\n",
      "k_h(r, r_i) = \\left\\{\n",
      "\\begin{matrix}\n",
      "\\frac{1}{n_h}, & r \\,\\text{is one of the $n_h$ closest points to $r_i$}\\\\\n",
      "0 & \\text{otherwise}.\n",
      "\\end{matrix}\n",
      "\\right.\n",
      "\\end{equation}\n",
      "\n",
      "Whether $r_i$ is one of the $n_h$ closest points to $r$ is determined by the metric on $\\mathcal{R}$.\n",
      "\n",
      "We can directly specify $n_h$ rather than a bandwidth to make calculations more straight forward, since specifying the number of data points to take into account for the kernel is a much more intuitive way to deal with the abstract distances of spike trains.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To use this kernel to do a KDE of the conditional probability density for the data, we must manipulate the expression for $p_{\\beta}(r_i | s_i)$, we start by doing the following,\n",
      "\n",
      "\\begin{align}\n",
      "p_{\\beta}(r_i | s_i) &= \\dfrac{p_{\\beta}(r_i, s_i)}{p_{\\beta}(s_i)} \\\\\n",
      "&= \\dfrac{p_{\\beta}(r_i, s_i)}{1/n_s} \\\\\n",
      "&= n_s p_{\\beta}(r_i, s_i),\n",
      "\\end{align}\n",
      "since we are assuming that the probability distribution of the stimuli is uniform. (**is it?**)\n",
      "\n",
      "So we will estimate $p_{\\beta}(r_i | s_i)$ through a kernel estiamtion of $p_{\\beta}(r_i, s_i)$, and our estimate is given by,\n",
      "\n",
      "\\begin{equation}\n",
      "\\tilde{p}_{\\beta}(r_i, s_i) = \\sum_{r \\in \\mathcal{R}_{s_i}} k(r, r_i, n_h),\n",
      "\\end{equation}\n",
      "where $\\mathcal{R}_{s_i}$ is the set of responses to stimulus $s_i$. This means that the sum of the kernels will add the number of responses to stimulus $s_i$ for which r_i is one of the nearest neighbours (one of the $n_h$ closest points).\n",
      "\n",
      "It is worth mentioning explicitly, that since the kernel radius is variable, since we are determining the volume of the support according to the new measure, that a point could be one of the $n_h$ nearest neighbours to points that are not *its own* nearest neighbours.\n",
      "\n",
      "So we can writhe the expression for our estimate in the following way:\n",
      "\\begin{equation}\n",
      "\\tilde{p}_{\\beta}(r_i, s_i) = \\dfrac{1}{n_h} c(r_i, s_i;n_h).\n",
      "\\end{equation}\n",
      "Here c(r_i, s_i;n_h) is the number of points that have $r_i$ as one of its nearest neighbours, when we are only considering spike trains that are due tu stimulus $s_i$ and being a nearest neightbour means on of the $n_h$ closest points.\n",
      "\n",
      "This means that,\n",
      "\\begin{align}\n",
      "p_{\\beta}(r_i|s_i) &= n_s p_{\\beta}(r_i,s_i) \\\\\n",
      "&\\approx \\dfrac{n_s c(r_i, s_i;n_h)}{n_h}.\n",
      "\\end{align}\n",
      "\n",
      "Therefore the estimated mutual information,\n",
      "\\begin{equation}\n",
      "\\tilde{I}(R,S;n_h) = \\dfrac{1}{n_r} \\sum_{i} \\log_2 \\dfrac{n_s c(r_i, s_i;n_h)}{n_h}\n",
      "\\end{equation}\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the previous expression for the estimate for the mutual information, writing the algorithm for calculating it is quite simple.\n",
      "Let us explicitly state the notation we have been using:\n",
      "\n",
      "   - $\\mathcal{S}$: The set of stimuli\n",
      "   - $s$: A point in the stiumulus set $\\mathcal{S}$\n",
      "   - $\\mathcal{R}$: The space of spike trains\n",
      "   - $r$: A spike train (a point in $\\mathcal{R}$)\n",
      "\n",
      "\n",
      "   - $n_s$: The number of different stimuli\n",
      "   - $n_t$: The number of trials each stimulus is presented\n",
      "   - $n_r$: The total number of responses (spike trains), $n_r = n_s n_t$\n",
      "\n",
      "\n",
      "   - $r_i$: An actual data point (spike train)\n",
      "   - $s_i$: An actual data point (stimulus)\n",
      "   \n",
      "   \n",
      "   - $S$: Random variable for stimulus\n",
      "   - $R$: Random variable for response\n",
      "   \n",
      "   - $h$: Bandwidth of kernel\n",
      "   - $n_h$: Number of neighbours bandwidth $h$ allows\n",
      "   \n",
      "It is worth noting that in the framework developed previously, each every stimulus is presented the same number of times as the others."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Set value for $n_h$\n",
      "2. Set I = 0\n",
      "3. For each stimulus $s_i$:\n",
      "    - $D_i$ = distance matrix between all $n_t$ $r_i$'s that correspond to $s_i$\n",
      "        - $c_{ij}$ = Number of entries in row j of $D_{ij}$ < $h$\n",
      "        - I +=  $\\log_2 \\dfrac{n_s c_{ij}}{n_h}$\n",
      "4. I *= $\\dfrac{1}{n_r}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Applying to data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sorting data into spike trains"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from parsing import *\n",
      "from spike_train_object import *\n",
      "\n",
      "spike_times = np.loadtxt(\"./data/Sfly01508SpikeTimes.txt\")\n",
      "repeated_exp = split_into_runs(spike_times, 200, 1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "198 runs were found\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have imported the spiking times for the neurons and split them into arrays corresponding to 5s repetitions of the same visual pattern.\n",
      "\n",
      "We want to be able to compare the spike trains for each run of the experiment, so the times must be equivalent. We now set all runs to start at $t=0$s"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "repeated_exp = [repeated_exp[i] - 5*i for i in range(len(repeated_exp))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since we only have one stimulus, we will split it into sections of 250ms in order to have a set of 20 stimuli. We also want to group the data by stimulus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "more_splitting = np.array([split_into_runs(run, 20, 5) for run in repeated_exp])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n",
        "20 runs were found\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The array `more_splitting` now contains, for each experiment run, the data for each of the twenty 250ms stimuli:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.shape(more_splitting)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 72,
       "text": [
        "(198, 20)"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "more_splitting[1][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 73,
       "text": [
        "array([ 0.0054,  0.0089,  0.0141,  0.0168,  0.0223,  0.06  ,  0.0748,\n",
        "        0.0772,  0.097 ,  0.0998,  0.1053,  0.1094,  0.1131,  0.1185,\n",
        "        0.1361,  0.1395,  0.1666,  0.1701,  0.2047,  0.2076,  0.2123,\n",
        "        0.2153,  0.221 ,  0.2309,  0.2332,  0.2358,  0.242 ,  0.2476])"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stimulus = [[SpikeTrain(more_splitting[j,i], start_time=i*0.05, end_time=(i+1)*0.05) for j in range(198)] \n",
      "            for i in range(20)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "stimulus is a list whose elements are the spike trains corresponding to each 250ms stimulus, so the 198th response to the 5th 50ms window is given below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stimulus[4][197].spiking_times"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "array([ 1.    ,  1.0036,  1.0064,  1.0101,  1.0146,  1.0181,  1.0223,\n",
        "        1.0279,  1.0464,  1.0509,  1.118 ,  1.1201,  1.1251,  1.1308,\n",
        "        1.1347,  1.1393,  1.1448,  1.1511,  1.1539,  1.157 ,  1.1643,\n",
        "        1.1729,  1.1833,  1.1896,  1.1918,  1.1966,  1.2102,  1.2122])"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have all the responses sorted by stimulus, which is the way we want our data structured, we must now run our algorithm to calculate the mutual information between $R$ and $S$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Calculating the mutual information"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Set value for $n_h$\n",
      "2. Set I = 0\n",
      "3. For each stimulus $s_i$:\n",
      "    - $D_i$ = distance matrix between all $n_t$ $r_i$'s that correspond to $s_i$\n",
      "        - $c_{ij}$ = Number of entries in row j of $D_{ij}$ < $h$\n",
      "        - I +=  $\\log_2 \\dfrac{n_s c_{ij}}{n_h}$\n",
      "4. I *= $\\dfrac{1}{n_r}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nh = "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}