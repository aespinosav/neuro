{
 "metadata": {
  "name": "",
  "signature": "sha256:324cc38bd785634cc1edc041b19674284fa3a4d291d53ef6d94a3dee42b61ea7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Information and entropy of spike trains"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from parsing import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Converting spiking times into words"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have loaded the spike times of the repeated experiment into a numpy array:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spike_times = np.loadtxt(\"Sfly01508SpikeTimes.txt\")\n",
      "spike_times"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "array([  1.80000000e-03,   1.29000000e-02,   1.85000000e-02, ...,\n",
        "         9.89965200e+02,   9.89970800e+02,   9.89978600e+02])"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We then split the whole timeseries into the respective experiment runs, since the last spike occurs at 989.7s we can assume that there are 2 experimental runs missing from this data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "repeated_exp = split_into_runs(spike_times, 200, 1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "198 runs were found\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next step is to convert the spike times into letters. To convert to letters we split the elapsed time into bins $\\Delta \\tau$ wide, for now we will use $\\Delta \\tau = 3$ms. Each experiment runs for 5s."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "experiments_letters = [convert_to_letters(i, 0.003, int(i[0]), 5) for i in repeated_exp]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have a list of arrays that contain the spike converted into letters (either 1's or 0's) for each time bin. To split the 5s experiments into letters we have only taken bins that are fully contained within the experiment time. That means that the last bin that overruns into the next experiment has been omitted."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "experiments_letters[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "array([0, 1, 1, ..., 0, 0, 0])"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(experiments_letters[0])*0.003"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "4.998"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the letters we now have, we can group them into words of length $T$. In our case we are using $T=30$ms which ammounts to 10-lettter words. Again we are only including words that are fully contained within the experiment time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "experiments_words = [convert_to_words(d, 10) for d in experiments_letters]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print experiments_words[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['1000101100', '1010000011', '0000111000', '0110111010', '0000100010', '0000110000', '0001001110', '1110101101', '0111000011', '0000000001', '1001110011', '0101010101', '1010100000', '0100000010', '1111001100', '1110000001', '0000000000', '0110001110', '0111011110', '1010110000', '0000000110', '0100000110', '0101000001', '1011101001', '0001011010', '1000000100', '0000000001', '0101000000', '0000000000', '1010100001', '1101010111', '0111010010', '1111011011', '1011110111', '0100000110', '0000000000', '0000011000', '0111011101', '1011100100', '0001011010', '0001000000', '0100000000', '0000000111', '1011010100', '0010000001', '1000000011', '0110100000', '0001100000', '0000000111', '1101011010', '0000011000', '0000100010', '0000010101', '0101101100', '1110101000', '0001110000', '0000000100', '0001110010', '0000000000', '0110010000', '0000000011', '1010100111', '0101111001', '0001011100', '0000110101', '0010110101', '1110100101', '1000011110', '1010000000', '0010111010', '1011000000', '0000100100', '0000000100', '0010010000', '0000011011', '1010001100', '0000000011', '1011011000', '0110101010', '0000111010', '1000000010', '0110001010', '0000001011', '1010000100', '0001010101', '0010110000', '1110000001', '0000110000', '0101010001', '0110010101', '0000000000', '0000000000', '0000000101', '0010000000', '0000000000', '0000000000', '0110010101', '0010000000', '0000000001', '0101110011', '1010010011', '0000000000', '0000000000', '1110110101', '0010101101', '0000000011', '0100000000', '0001001000', '0000001010', '0011010110', '1001000011', '1011011101', '1001000000', '0100000000', '0000000000', '0000000000', '0000000000', '0000000110', '1000110110', '1101101001', '0110100000', '0000000000', '0000100100', '0000101000', '1001010110', '1110110100', '1000000000', '0000100000', '1000110110', '1000000000', '0101100000', '0110101101', '0010000000', '0001101000', '0010100110', '0111010100', '0100000000', '0000000000', '0111010000', '1110111001', '0101100000', '0001101101', '1010000001', '1101010110', '1000000000', '0100000000', '0000010000', '0001010010', '0000000000', '0000000000', '0100011000', '0000000011', '0011010000', '0000000010', '1010101000', '0110111000', '0001100000', '1011101000', '0000000001', '0001001110', '1000000000', '0001000000', '0011100000', '0000010100', '1001100001', '1001101010']\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(experiments_words[0])*0.03"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "4.9799999999999995"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are only taking 4.98s of the experiment into account."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Converting word lists into distributions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once the spike time data has been converted into words for each experiment we need to make distribution form these word lists. With these distributions we will be able to calculate entropies."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "experiment_distributions = [word_distribution(e) for e in experiments_words]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of these distributions is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "experiment_distributions[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "{'0000000000': 24,\n",
        " '0000000001': 5,\n",
        " '0000000010': 3,\n",
        " '0000000011': 6,\n",
        " '0000000100': 1,\n",
        " '0000000101': 1,\n",
        " '0000000110': 2,\n",
        " '0000000111': 1,\n",
        " '0000001000': 1,\n",
        " '0000001001': 1,\n",
        " '0000001110': 1,\n",
        " '0000010000': 2,\n",
        " '0000011000': 3,\n",
        " '0000011010': 1,\n",
        " '0000011101': 1,\n",
        " '0000100000': 3,\n",
        " '0000101000': 2,\n",
        " '0000101101': 1,\n",
        " '0000110000': 2,\n",
        " '0000110100': 1,\n",
        " '0000111001': 2,\n",
        " '0001000000': 1,\n",
        " '0001000011': 2,\n",
        " '0001010000': 1,\n",
        " '0001100000': 1,\n",
        " '0001101000': 2,\n",
        " '0001101011': 1,\n",
        " '0001101101': 1,\n",
        " '0001101110': 2,\n",
        " '0001110100': 1,\n",
        " '0001110110': 1,\n",
        " '0010000000': 1,\n",
        " '0010001100': 1,\n",
        " '0010100000': 1,\n",
        " '0010100101': 1,\n",
        " '0010110000': 1,\n",
        " '0011000000': 2,\n",
        " '0011000010': 1,\n",
        " '0011010011': 1,\n",
        " '0011011101': 1,\n",
        " '0011100000': 1,\n",
        " '0011100100': 1,\n",
        " '0011100111': 1,\n",
        " '0011101110': 1,\n",
        " '0011110010': 1,\n",
        " '0011110101': 1,\n",
        " '0100000000': 5,\n",
        " '0100000001': 1,\n",
        " '0100010011': 1,\n",
        " '0100011000': 1,\n",
        " '0100100000': 2,\n",
        " '0100110110': 1,\n",
        " '0101000000': 2,\n",
        " '0101010000': 1,\n",
        " '0101011010': 2,\n",
        " '0101100000': 1,\n",
        " '0101110100': 1,\n",
        " '0101110111': 1,\n",
        " '0110011010': 1,\n",
        " '0110100000': 1,\n",
        " '0110101110': 1,\n",
        " '0110110100': 1,\n",
        " '0110111010': 1,\n",
        " '0111010000': 1,\n",
        " '0111010100': 2,\n",
        " '0111101010': 1,\n",
        " '0111101111': 1,\n",
        " '1000000000': 2,\n",
        " '1000000010': 2,\n",
        " '1000000110': 1,\n",
        " '1000001011': 1,\n",
        " '1001000000': 1,\n",
        " '1001001110': 1,\n",
        " '1001100000': 1,\n",
        " '1010000000': 1,\n",
        " '1010000001': 1,\n",
        " '1010001001': 1,\n",
        " '1010100001': 1,\n",
        " '1010100010': 1,\n",
        " '1010110100': 1,\n",
        " '1010110101': 1,\n",
        " '1010110110': 1,\n",
        " '1010111001': 1,\n",
        " '1011010000': 1,\n",
        " '1011010100': 1,\n",
        " '1011010110': 1,\n",
        " '1011010111': 1,\n",
        " '1011101010': 2,\n",
        " '1011111101': 1,\n",
        " '1100100000': 1,\n",
        " '1100100101': 1,\n",
        " '1100101100': 1,\n",
        " '1101000110': 1,\n",
        " '1101001110': 1,\n",
        " '1101010000': 1,\n",
        " '1101010100': 1,\n",
        " '1101011010': 1,\n",
        " '1101101010': 1,\n",
        " '1101101100': 1,\n",
        " '1101101110': 1,\n",
        " '1101110100': 2,\n",
        " '1101110111': 1,\n",
        " '1110000000': 1,\n",
        " '1110010001': 1,\n",
        " '1110101001': 1,\n",
        " '1110110100': 1,\n",
        " '1111001001': 1}"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that the most common word is $0000000000$. We also want distributions for the frequency of words that occur at a certain time throughout all the repeated experiments to measure $S_\\text{noise}$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "time_distributions = word_distributions_given_t(experiments_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This gives us the frecuency distributions for words given time, since we have already partitioned the experiment into words the times we are interested are integer multiples of $T$ which in our case is 30ms. The distributions show interesting behaviour, for certain times the number of distinct words is very low. For example, at 30ms throughout the 198 experiments we only observe 4 different words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "time_distributions[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "{'0000000000': 184, '0000000001': 1, '1000000000': 12, '1010000011': 1}"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Entropy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From these distributions we will now calculate the entropy,\n",
      "\n",
      "$$\n",
      "S = - \\sum p_i \\log_2 p_i,\n",
      "$$\n",
      "\n",
      "where $p_i$ is the normalised frequency of the words. \n",
      "\n",
      "We will calculate the average entropy for the 198 experiment runs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "entropy_total = [calculate_entropy(experiment_distributions[i]) for i in range(len(experiment_distributions))]\n",
      "ave_total_entropy = np.average(entropy_total)\n",
      "\n",
      "ave_total_entropy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "5.7568330475612388"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To measure the variation of words given the stimulus we calculate the entropies of each of the distributions for each of time step of 30ms through all the experiments. And we average that to get $S_\\text{noise}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "entropies_t = [calculate_entropy(e) for e in time_distributions]\n",
      "noise_entropy = np.average(entropies_t)\n",
      "\n",
      "noise_entropy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "2.6998731063567902"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Therefore we find that:\n",
      "\n",
      "- $S_\\text{total} = 5.76$\n",
      "- $S_\\text{noise} = 2.70$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which differs only slightly form the results given in the paper by de Ruyter van Steveninck *et al* (1997). In which they find:\n",
      "\n",
      "- $S_\\text{total} = 5.05 \\pm 0.01$\n",
      "- $S_\\text{noise} = 2.62 \\pm 0.02$\n",
      "\n",
      "These differences might be due in part to the fact that we are not allowing time for the transient spiking to occur before we convert spiking times into words.\n",
      "\n",
      "We will now calculate $S_\\text{total}$ from the 1000s experiment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spike_times2 = np.loadtxt(\"Sfly01509SpikeTimes.txt\")\n",
      "exp2_letters = convert_to_letters(spike_times2, 0.003, 0, 1000)\n",
      "exp2_words = convert_to_words(exp2_letters, 10)\n",
      "exp2_dist = word_distribution(exp2_words)\n",
      "\n",
      "calculate_entropy(exp2_dist)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "7.0721645919556382"
       ]
      }
     ],
     "prompt_number": 49
    }
   ],
   "metadata": {}
  }
 ]
}